{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "760f803d-3716-4756-b16c-34eed6b05b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n",
      "D:\\MyCodes\\Projects\\YetiCTF\\MNISTED - Not tested\\dev\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchmetrics\n",
    "import timm\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from PIL import Image\n",
    "import requests\n",
    "import os\n",
    "print(torch.__version__)\n",
    "CURRENT_PATH = os.getcwd()\n",
    "print(CURRENT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dded1bb6-80ae-4a42-8353-91fa8ea7971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs import DefaultTrainer, DefaultGym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3dfb2a4-c79d-4dfe-b915-7933d71f75d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "train_mean, train_std = 0.1307, 0.3081\n",
    "BATCH_SIZE = 100\n",
    "CLASSES = range(10)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  datasets.MNIST(f'{CURRENT_PATH}/dataset/mnist', train=True, download=True, transform = transforms.Compose([\n",
    "                    transforms.RandomAffine(degrees=20, translate=(0.1,0.1), scale=(0.9, 1.1)),\n",
    "                    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[train_mean], std=[train_std]),\n",
    "                    ])),\n",
    "  batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "  datasets.MNIST(f'{CURRENT_PATH}/dataset/mnist', train=False, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[train_mean], std=[train_std]),\n",
    "                    ])),\n",
    "  batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3514b372-ac92-4816-a1ee-c93a125fbcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train - 600 btchs | 60000 imgs\n",
      "valid - 100 btchs | 10000 imgs\n"
     ]
    }
   ],
   "source": [
    "print(f\"train - {len(train_loader)} btchs | {len(train_loader.dataset)} imgs\")\n",
    "print(f\"valid - {len(valid_loader)} btchs | {len(valid_loader.dataset)} imgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85e4d9e2-3d70-4b17-ab2e-8cea248332d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.4242)\n",
      "tensor(2.8215)\n",
      "tensor(-0.0043)\n",
      "tensor(0.9533)\n",
      "8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# sanity check for training data\n",
    "imgs, lbls = next(iter(train_loader))\n",
    "imgs[7].data.shape\n",
    "print(imgs.data.min())\n",
    "print(imgs.data.max())\n",
    "print(imgs.data.mean())\n",
    "print(imgs.data.std())\n",
    "print(CLASSES[lbls[0]])\n",
    "plt.imshow(imgs[0].data.reshape((28,28)), cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6934734c-28f1-4928-9ae8-29ef27e95bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.4242)\n",
      "tensor(2.8215)\n",
      "tensor(-0.0351)\n",
      "tensor(0.9603)\n",
      "7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sanity check for validation data\n",
    "imgs, lbls = next(iter(valid_loader))\n",
    "imgs[0].data.shape\n",
    "print(imgs.data.min())\n",
    "print(imgs.data.max())\n",
    "print(imgs.data.mean())\n",
    "print(imgs.data.std())\n",
    "print(CLASSES[lbls[0]])\n",
    "plt.imshow(imgs[0].data.reshape((28,28)), cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97243b28-cfaa-4236-85e6-658fe081177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model definition\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## deeper model adapted from https://www.kaggle.com/gustafsilva/cnn-digit-recognizer-pytorch\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "                \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1eb7d63-11f9-4ec2-8d6e-13cc2e13fd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7deed41-7ee4-463a-9e71-7acd17814505",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gym = DefaultGym(\n",
    "    num_of_epochs=25,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    optimizer_wrap=lambda model: torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-1),\n",
    "    scheduler_wrap=lambda optimizer: torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95),\n",
    "    metrics=[torchmetrics.MetricCollection([\n",
    "                torchmetrics.Precision(num_classes=10, average='macro'),\n",
    "                torchmetrics.Recall(num_classes=10, average='macro')])] * 2,\n",
    "    target_metric_key=\"Precision\",\n",
    "    log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e024e524-1d80-4845-a554-afc9845adc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trainer = DefaultTrainer(dataloaders={\"train\": train_loader, \"valid\": valid_loader},\n",
    "                            gyms=[my_gym])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad4e65fd-970f-48ca-a094-441c70fbc4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing the training room(<class 'libs.DefaultGym'>) for the model(<class '__main__.Model'>)\n",
      "    Options of gym:\n",
      "        * criterion - <class 'torch.nn.modules.loss.CrossEntropyLoss'>;\n",
      "        * optimizer - {'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False};\n",
      "        * scheduler - {'step_size': 1, 'gamma': 0.95, 'optimizer': Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.1\n",
      "    lr: 0.1\n",
      "    weight_decay: 0\n",
      "), 'base_lrs': [0.1], 'last_epoch': 0, '_step_count': 1, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.1]};\n",
      "\n",
      "        Metrics:\n",
      "            train: MetricCollection(\n",
      "  (Precision): Precision()\n",
      "  (Recall): Recall()\n",
      ")\n",
      "            valid: MetricCollection(\n",
      "  (Precision): Precision()\n",
      "  (Recall): Recall()\n",
      ")\n",
      "\n",
      "    Dataloaders:\n",
      "            train: shape - 600 btchs | 60000 imgs; transforms - ;\n",
      "            valid: shape - 100 btchs | 10000 imgs; transforms - ;\n",
      "                    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de874faffc474ff2801b3cce75c570c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs: 0epochs [00:00, ?epochs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c991302d1254084ad398b7bed12222b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid steps: 0steps [00:00, ?steps/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5794f698e4b4669a305009a0ab551c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train steps: 0steps [00:00, ?steps/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation score(Precision) has improved from 0.0 to 0.979338526725769... Saving model.\n",
      "The validation score(Precision) has improved from 0.979338526725769 to 0.9827855825424194... Saving model.\n",
      "The validation score(Precision) has improved from 0.9827855825424194 to 0.9869351983070374... Saving model.\n",
      "The validation score(Precision) has improved from 0.9869351983070374 to 0.9905048608779907... Saving model.\n",
      "The validation score(Precision) has improved from 0.9905048608779907 to 0.9907800555229187... Saving model.\n",
      "The validation score(Precision) has improved from 0.9907800555229187 to 0.9911941289901733... Saving model.\n",
      "The validation score(Precision) has improved from 0.9911941289901733 to 0.992155909538269... Saving model.\n",
      "The validation score(Precision) has improved from 0.992155909538269 to 0.9928884506225586... Saving model.\n",
      "The validation score(Precision) has improved from 0.9928884506225586 to 0.994999885559082... Saving model.\n",
      "The validation score(Precision) has improved from 0.994999885559082 to 0.9954765439033508... Saving model.\n",
      "The validation score(Precision) has improved from 0.9954765439033508 to 0.9954943656921387... Saving model.\n"
     ]
    }
   ],
   "source": [
    "trained = my_trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4b9adcf-5591-42c8-a68f-64f62d3d9fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(torch.jit.script(model), 'anal-model-jit-script.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a76fa5-4995-4a2d-b8b3-3d3b54882f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
